{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMx3v1pNDhR/4f0Gcjj8kwv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ranabilal09/Skeleton-of-Thoughts/blob/main/Skeleton-of-Thoughts.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade --quiet langchain langchain_community langchain-google-genai"
      ],
      "metadata": {
        "id": "36TYA50znt9P"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Re7AbJnynpm8",
        "outputId": "18fbed24-95c0-4014-d0e8-0ac596b75e94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py:3553: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
            "\n",
            "For example, replace imports like: `from langchain.pydantic_v1 import BaseModel`\n",
            "with: `from pydantic import BaseModel`\n",
            "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
            "\n",
            "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
          ]
        }
      ],
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.pydantic_v1 import BaseModel\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "\n",
        "skeleton_generator_template = \"\"\"[User:] You’re an organizer responsible for only \\\n",
        "giving the skeleton (not the full content) for answering the question.\n",
        "Provide the skeleton in a list of points (numbered 1., 2., 3., etc.) to answer \\\n",
        "the question. \\\n",
        "Instead of writing a full sentence, each skeleton point should be very short \\\n",
        "with only 3∼5 words. \\\n",
        "Generally, the skeleton should have 3∼10 points. Now, please provide the skeleton \\\n",
        "for the following question.\n",
        "{question}\n",
        "Skeleton:\n",
        "[Assistant:] 1.\"\"\"\n",
        "\n",
        "skeleton_generator_prompt = ChatPromptTemplate.from_template(\n",
        "    skeleton_generator_template\n",
        ")\n",
        "\n",
        "skeleton_generator_chain = (\n",
        "    skeleton_generator_prompt | ChatGoogleGenerativeAI(model=\"gemini-1.5-flash-8b\") | StrOutputParser() | (lambda x: \"1. \" + x)\n",
        ")\n",
        "\n",
        "point_expander_template = \"\"\"[User:] You’re responsible for continuing \\\n",
        "the writing of one and only one point in the overall answer to the following question.\n",
        "{question}\n",
        "The skeleton of the answer is\n",
        "{skeleton}\n",
        "Continue and only continue the writing of point {point_index}. \\\n",
        "Write it **very shortly** in 1∼2 sentence and do not continue with other points!\n",
        "[Assistant:] {point_index}. {point_skeleton}\"\"\"\n",
        "\n",
        "point_expander_prompt = ChatPromptTemplate.from_template(point_expander_template)\n",
        "\n",
        "point_expander_chain = RunnablePassthrough.assign(\n",
        "    continuation=point_expander_prompt | ChatGoogleGenerativeAI(model=\"gemini-1.5-flash-8b\") | StrOutputParser()\n",
        ") | (lambda x: x[\"point_skeleton\"].strip() + \" \" + x[\"continuation\"])\n",
        "\n",
        "\n",
        "def parse_numbered_list(input_str):\n",
        "    \"\"\"Parses a numbered list into a list of dictionaries\n",
        "\n",
        "    Each element having two keys:\n",
        "    'index' for the index in the numbered list, and 'point' for the content.\n",
        "    \"\"\"\n",
        "    # Split the input string into lines\n",
        "    lines = input_str.split(\"\\n\")\n",
        "\n",
        "    # Initialize an empty list to store the parsed items\n",
        "    parsed_list = []\n",
        "\n",
        "    for line in lines:\n",
        "        # Split each line at the first period to separate the index from the content\n",
        "        parts = line.split(\". \", 1)\n",
        "\n",
        "        if len(parts) == 2:\n",
        "            # Convert the index part to an integer\n",
        "            # and strip any whitespace from the content\n",
        "            index = int(parts[0])\n",
        "            point = parts[1].strip()\n",
        "\n",
        "            # Add a dictionary to the parsed list\n",
        "            parsed_list.append({\"point_index\": index, \"point_skeleton\": point})\n",
        "\n",
        "    return parsed_list\n",
        "\n",
        "\n",
        "def create_list_elements(_input):\n",
        "    skeleton = _input[\"skeleton\"]\n",
        "    numbered_list = parse_numbered_list(skeleton)\n",
        "    for el in numbered_list:\n",
        "        el[\"skeleton\"] = skeleton\n",
        "        el[\"question\"] = _input[\"question\"]\n",
        "    return numbered_list\n",
        "\n",
        "\n",
        "def get_final_answer(expanded_list):\n",
        "    final_answer_str = \"Here's a comprehensive answer:\\n\\n\"\n",
        "    for i, el in enumerate(expanded_list):\n",
        "        final_answer_str += f\"{i+1}. {el}\\n\\n\"\n",
        "    return final_answer_str\n",
        "\n",
        "\n",
        "class ChainInput(BaseModel):\n",
        "    question: str\n",
        "\n",
        "\n",
        "chain = (\n",
        "    RunnablePassthrough.assign(skeleton=skeleton_generator_chain)\n",
        "    | create_list_elements\n",
        "    | point_expander_chain.map()\n",
        "    | get_final_answer\n",
        ").with_types(input_type=ChainInput)"
      ]
    }
  ]
}